{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3rdEZuu17ph"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer()\n",
        "x_raw = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Z-score Normalization\n",
        "mean = np.mean(x_raw, axis=0)\n",
        "std = np.std(x_raw, axis=0)\n",
        "\n",
        "# Avoid division by zero in case of constant features\n",
        "std[std == 0] = 1.0\n",
        "\n",
        "x_normalized = (x_raw - mean) / std"
      ],
      "metadata": {
        "id": "b2O64FEi3p8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCA:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components = None\n",
        "        self.mean = None\n",
        "        self.eigenvalues = None\n",
        "        self.explained_variance_ratio = None\n",
        "\n",
        "    def fit(self, x):\n",
        "        # Mean centering\n",
        "        self.mean = np.mean(x, axis=0)\n",
        "        x_centered = x - self.mean\n",
        "\n",
        "        # np.cov(x.T) expects rows as features, so we use transpose\n",
        "        covariance_matrix = np.cov(x_centered.T)\n",
        "\n",
        "        # Eigenvalue Decomposition\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
        "\n",
        "        # Sort eigenvalues and eigenvectors (descending)\n",
        "        # Greater the eigenvalue the greater the variance captured\n",
        "        indices = np.argsort(eigenvalues)[::-1]\n",
        "        self.eigenvalues = eigenvalues[indices]\n",
        "        sorted_eigenvectors = eigenvectors[:, indices]\n",
        "\n",
        "        # Select top n_components\n",
        "        self.components = sorted_eigenvectors[:, 0:self.n_components]\n",
        "\n",
        "        # Calculate Explained Variance Ratio\n",
        "        total_variance = np.sum(self.eigenvalues)\n",
        "        self.explained_variance_ratio = (self.eigenvalues[:self.n_components] / total_variance)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "        # Project data\n",
        "        return np.dot(x - self.mean, self.components)\n",
        "\n",
        "    def inverse_transform(self, x_transformed):\n",
        "        return np.dot(x_transformed, self.components.T) + self.mean\n",
        "\n",
        "    def calculate_reconstruction_error(self, x):\n",
        "        x_transformed = self.transform(x)\n",
        "        x_reconstructed = self.inverse_transform(x_transformed)\n",
        "        mean_squarred_error = np.mean((x - x_reconstructed) ** 2)\n",
        "        return mean_squarred_error"
      ],
      "metadata": {
        "id": "qOXigtFc5Hgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GMM:\n",
        "    def __init__(self, n_components, covariance_type='full', convergence_threshold=1e-3, max_iter=100, regularized_covariance=1e-6):\n",
        "        self.n_components = n_components\n",
        "        self.covariance_type = covariance_type\n",
        "        self.convergence_threshold = convergence_threshold\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "        # Regularization to prevent singular matrices\n",
        "        self.regularized_covariance = regularized_covariance\n",
        "\n",
        "        # Parameters to be learned\n",
        "        self.means = None\n",
        "        self.covariances = None\n",
        "        self.weights = None\n",
        "        self.log_likelihood_history = []\n",
        "        self.converged = False\n",
        "\n",
        "    def _initialize_parameters(self, x):\n",
        "        n_samples, n_features = x.shape\n",
        "\n",
        "        # Initialize Weights (uniform)\n",
        "        self.weights = np.full(self.n_components, 1 / self.n_components)\n",
        "\n",
        "        # Initialize Means randomly\n",
        "        indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
        "        self.means = x[indices]\n",
        "\n",
        "        # Initialize Covariances based on type\n",
        "        if self.covariance_type == 'full':\n",
        "            self.covariances = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
        "        elif self.covariance_type == 'tied':\n",
        "            self.covariances = np.eye(n_features)\n",
        "        elif self.covariance_type == 'diagonal':\n",
        "            self.covariances = np.ones((self.n_components, n_features))\n",
        "        elif self.covariance_type == 'spherical':\n",
        "            self.covariances = np.ones(self.n_components)\n",
        "\n",
        "    def _estimate_gaussian_log_probability(self, x):\n",
        "        n_samples, n_features = x.shape\n",
        "        log_probabilities = np.zeros((n_samples, self.n_components))\n",
        "\n",
        "        for k in range(self.n_components):\n",
        "            mean = self.means[k]\n",
        "\n",
        "            # Compute log-probability based on covariance type\n",
        "            if self.covariance_type == 'full':\n",
        "                # Add regularization to diagonal\n",
        "                covariance_matrix = self.covariances[k] + np.eye(n_features) * self.regularized_covariance\n",
        "                try:\n",
        "                    # Cholesky decomposition for stable determinant and inverse\n",
        "                    lower_triangular = np.linalg.cholesky(covariance_matrix)\n",
        "                    log_determinant = 2 * np.sum(np.log(np.diag(lower_triangular)))\n",
        "                    precision_matrix = np.linalg.inv(covariance_matrix)\n",
        "\n",
        "                    difference = x - mean\n",
        "                    # Mahalanobis distance calculation (x-mu) * inv(cov) * (x-mu)^T\n",
        "                    temp = np.dot(difference, precision_matrix)\n",
        "                    mahalanobis_distance = np.sum(temp * difference, axis=1)\n",
        "\n",
        "                    log_probabilities[:, k] = -0.5 * (n_features * np.log(2 * np.pi) + log_determinant + mahalanobis_distance)\n",
        "                except np.linalg.LinAlgError:\n",
        "                    log_probabilities[:, k] = -np.inf # Handle singular matrix\n",
        "\n",
        "            elif self.covariance_type == 'tied':\n",
        "                covariance_matrix = self.covariances + np.eye(n_features) * self.regularized_covariance\n",
        "                try:\n",
        "                    lower_triangular = np.linalg.cholesky(covariance_matrix)\n",
        "                    log_determinant = 2 * np.sum(np.log(np.diag(lower_triangular)))\n",
        "                    precision_matrix = np.linalg.inv(covariance_matrix)\n",
        "\n",
        "                    difference = x - mean\n",
        "                    temp = np.dot(difference, precision_matrix)\n",
        "                    mahalanobis_distance = np.sum(temp * difference, axis=1)\n",
        "\n",
        "                    log_probabilities[:, k] = -0.5 * (n_features * np.log(2 * np.pi) + log_determinant + mahalanobis_distance)\n",
        "                except np.linalg.LinAlgError:\n",
        "                    log_probabilities[:, k] = -np.inf\n",
        "\n",
        "            elif self.covariance_type == 'diagonal':\n",
        "                covariance_vector = self.covariances[k] + self.regularized_covariance\n",
        "                log_determinant = np.sum(np.log(covariance_vector))\n",
        "\n",
        "                difference = x - mean\n",
        "                mahalanobis_distance = np.sum((difference ** 2) / covariance_vector, axis=1)\n",
        "\n",
        "                log_probabilities[:, k] = -0.5 * (n_features * np.log(2 * np.pi) + log_determinant + mahalanobis_distance)\n",
        "\n",
        "            elif self.covariance_type == 'spherical':\n",
        "                covariance_scalar = self.covariances[k] + self.regularized_covariance\n",
        "                log_determinant = n_features * np.log(covariance_scalar)\n",
        "\n",
        "                difference = x - mean\n",
        "                mahalanobis_distance = np.sum((difference ** 2), axis=1) / covariance_scalar\n",
        "\n",
        "                log_probabilities[:, k] = -0.5 * (n_features * np.log(2 * np.pi) + log_determinant + mahalanobis_distance)\n",
        "\n",
        "        return log_probabilities\n",
        "\n",
        "    def _e_step(self, x):\n",
        "        # Calculate log probabilities: log(P(x|theta))\n",
        "        log_probabilities = self._estimate_gaussian_log_probability(x)\n",
        "\n",
        "        # Weighted log probabilities: log(w_k) + log(P(x|theta))\n",
        "        weighted_log_probabilities = log_probabilities + np.log(self.weights + 1e-10)\n",
        "\n",
        "        # Log-Sum-Exp trick for numerical stability\n",
        "        max_weighted_log_prob = np.max(weighted_log_probabilities, axis=1, keepdims=True)\n",
        "\n",
        "        # Use consistent variable name\n",
        "        log_responsibilities = weighted_log_probabilities - max_weighted_log_prob\n",
        "        log_responsibilities = log_responsibilities - np.log(np.sum(np.exp(log_responsibilities), axis=1, keepdims=True))\n",
        "\n",
        "        # Responsibilities\n",
        "        responsibilities = np.exp(log_responsibilities)\n",
        "        return responsibilities, weighted_log_probabilities\n",
        "\n",
        "    def _m_step(self, x, responsibilities):\n",
        "        n_samples, n_features = x.shape\n",
        "        weights_sum = np.sum(responsibilities, axis=0) # Nk\n",
        "\n",
        "        # Update Weights\n",
        "        self.weights = weights_sum / n_samples\n",
        "\n",
        "        # Added 1e-10 to the denominator to prevent DivisionByZero errors if a cluster becomes empty (has 0 responsibilities) during training\n",
        "        safe_weights_sum = weights_sum[:, np.newaxis] + 1e-10\n",
        "\n",
        "        # Update Means\n",
        "        self.means = np.dot(responsibilities.T, x) / safe_weights_sum\n",
        "\n",
        "        # 3. Update Covariances\n",
        "        if self.covariance_type == 'full':\n",
        "            for k in range(self.n_components):\n",
        "                difference = x - self.means[k]\n",
        "                weighted_difference = responsibilities[:, k][:, np.newaxis] * difference\n",
        "                self.covariances[k] = np.dot(weighted_difference.T, difference) / (weights_sum[k] + 1e-10)\n",
        "\n",
        "        elif self.covariance_type == 'tied':\n",
        "            average_covariance = np.zeros((n_features, n_features))\n",
        "            for k in range(self.n_components):\n",
        "                difference = x - self.means[k]\n",
        "                weighted_difference = responsibilities[:, k][:, np.newaxis] * difference\n",
        "                average_covariance += np.dot(weighted_difference.T, difference)\n",
        "            self.covariances = average_covariance / n_samples\n",
        "\n",
        "        elif self.covariance_type == 'diagonal':\n",
        "            for k in range(self.n_components):\n",
        "                difference = x - self.means[k]\n",
        "                # Only keep diagonal elements (variance)\n",
        "                self.covariances[k] = np.sum(responsibilities[:, k][:, np.newaxis] * (difference ** 2), axis=0) / (weights_sum[k] + 1e-10)\n",
        "\n",
        "        elif self.covariance_type == 'spherical':\n",
        "            for k in range(self.n_components):\n",
        "                difference = x - self.means[k]\n",
        "                # Average variance across all features\n",
        "                variance = np.sum(responsibilities[:, k][:, np.newaxis] * (difference ** 2)) / (weights_sum[k] + 1e-10)\n",
        "                self.covariances[k] = variance / n_features\n",
        "\n",
        "    def fit(self, x):\n",
        "        self._initialize_parameters(x)\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            prev_log_likelihood = self.log_likelihood_history[-1] if self.log_likelihood_history else -np.inf\n",
        "\n",
        "            # Expectation Step\n",
        "            responsibilities, weighted_log_probabilities = self._e_step(x)\n",
        "\n",
        "            # Maximization Step\n",
        "            self._m_step(x, responsibilities)\n",
        "\n",
        "            # Compute Log-Likelihood\n",
        "            max_weighted_log_prob = np.max(weighted_log_probabilities, axis=1)\n",
        "            sum_exponential = np.sum(np.exp(weighted_log_probabilities - max_weighted_log_prob[:, np.newaxis]), axis=1)\n",
        "            log_likelihood = np.sum(max_weighted_log_prob + np.log(sum_exponential))\n",
        "\n",
        "            self.log_likelihood_history.append(log_likelihood)\n",
        "\n",
        "            # Convergence Check\n",
        "            if abs(log_likelihood - prev_log_likelihood) < self.convergence_threshold:\n",
        "                self.converged = True\n",
        "                break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        responsibilities, _ = self._e_step(x)\n",
        "        return np.argmax(responsibilities, axis=1)\n",
        "\n",
        "    def bic(self, x):\n",
        "        # Bayesian Information Criterion (Lower is better)\n",
        "        n_samples, n_features = x.shape\n",
        "        log_likelihood = self.log_likelihood_history[-1]\n",
        "\n",
        "        # Count parameters based on covariance type\n",
        "        if self.covariance_type == 'full':\n",
        "            covariance_parameters = self.n_components * n_features * (n_features + 1) / 2\n",
        "        elif self.covariance_type == 'tied':\n",
        "            covariance_parameters = n_features * (n_features + 1) / 2\n",
        "        elif self.covariance_type == 'diagonal':\n",
        "            covariance_parameters = self.n_components * n_features\n",
        "        elif self.covariance_type == 'spherical':\n",
        "            covariance_parameters = self.n_components\n",
        "\n",
        "        n_parameters = (self.n_components * n_features) + covariance_parameters + (self.n_components - 1)\n",
        "\n",
        "        return -2 * log_likelihood + n_parameters * np.log(n_samples)\n",
        "\n",
        "    def aic(self, x):\n",
        "        # Akaike Information Criterion (Lower is better)\n",
        "        n_samples, n_features = x.shape\n",
        "        log_likelihood = self.log_likelihood_history[-1]\n",
        "\n",
        "        # Calculate params\n",
        "        if self.covariance_type == 'full':\n",
        "            covariance_parameters = self.n_components * n_features * (n_features + 1) / 2\n",
        "        elif self.covariance_type == 'tied':\n",
        "            covariance_parameters = n_features * (n_features + 1) / 2\n",
        "        elif self.covariance_type == 'diagonal':\n",
        "            covariance_parameters = self.n_components * n_features\n",
        "        elif self.covariance_type == 'spherical':\n",
        "            covariance_parameters = self.n_components\n",
        "\n",
        "        n_parameters = (self.n_components * n_features) + covariance_parameters + (self.n_components - 1)\n",
        "\n",
        "        return -2 * log_likelihood + 2 * n_parameters"
      ],
      "metadata": {
        "id": "HRb2Q7EGD7W1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}